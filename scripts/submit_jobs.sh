#!/bin/bash

# ===================================
# CoinCap ETL Pipeline Automation
# ===================================

# Exit immediately if any command fails
set -e

# -------------------------------
# 1Ô∏è‚É£ Load environment variables safely
# -------------------------------
if [ -f ".env" ]; then
    echo "üîπ Loading environment variables from .env..."
    set -a           # automatically export all variables
    source .env
    set +a
else
    echo "‚ùå .env file not found! Please create one."
    exit 1
fi

# Verify critical variables
echo "üåê Using GCP Project: $GCP_PROJECT_ID"
echo "üóÇ Dataproc Cluster: $DATAPROC_CLUSTER in $DATAPROC_REGION"
echo "üì¶ GCS Bucket: $GCS_BUCKET"
echo "üîë Pub/Sub Topic: $PUBSUB_TOPIC"
echo "üîë Pub/Sub Subscription: $PUBSUB_SUBSCRIPTION"

# Set GCP credentials
export GOOGLE_APPLICATION_CREDENTIALS=$GCP_SERVICE_ACCOUNT_KEY

# -------------------------------
# 2Ô∏è‚É£ Batch Pipeline
# -------------------------------
echo "üöÄ Starting batch pipeline: GCS -> BigQuery"

gcloud dataproc jobs submit pyspark batch/batch_assets_to_bq.py \
    --cluster "$DATAPROC_CLUSTER" \
    --region "$DATAPROC_REGION" \
    --project "$GCP_PROJECT_ID" \
    --properties spark.jars.packages=com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.30.0

echo "‚úÖ Batch pipeline completed"

# -------------------------------
# 3Ô∏è‚É£ Streaming Pipeline
# -------------------------------
echo "üöÄ Starting streaming pipeline: Pub/Sub -> BigQuery (background)"

# Run streaming pipeline in background safely
(
gcloud dataproc jobs submit pyspark streaming/stream_assets_to_bq.py \
    --cluster "$DATAPROC_CLUSTER" \
    --region "$DATAPROC_REGION" \
    --project "$GCP_PROJECT_ID" \
    --properties spark.jars.packages=com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.30.0
) &

STREAM_JOB_PID=$!
echo "‚ÑπÔ∏è Streaming job submitted in background (PID: $STREAM_JOB_PID)"

echo "‚úÖ Automation script finished. Streaming is running in background."
